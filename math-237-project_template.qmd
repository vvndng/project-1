---
title: "Eenie Meenie Miney Mo - Where Should I Go?"
author: "Vivian Duong"
---

## Motivation and Context

As college students, many of us may choose to move out in the near future or are currently searching for places to stay for work, school, or personal reasons. Searching for a place is hard enough as it is already so with this project, I wanted to relieve some of the stress and perform some of the work with R, especially the math. Since I intend to have this project be used for my own personal reasons, I hope that it can be used to help make me make decisions easier.

The data for this project includes the following variables: the monthly rent (\$), number of bedrooms, number of bathrooms, the square footage of the housing unit, and its type. The full address and the county where the listing is located will also be included for reference.

If the listing indicates "0 beds" AND is an apartment, it is considered a studio apartment. In my experience, it's not unusual for a condo to share the living room, bedroom, and kitchen in one space but listings that are a house AND indicate "0 beds" should be investigated.

```{r}
#| label: do this first
#| echo: false
#| message: false

here::i_am("project1/math-237-project_template.qmd")
```

## Main Objective

The main objective is to find the "best deal" of rental listings in the context of being an undergraduate student. For this project, I define "best deal" as a listing having negative residuals for monthly rent price compared to its expected values based on a linear regression model.

## Packages Used In This Analysis

```{r}
#| label: load packages
#| message: false
#| warning: false

library(here)
library(tidyverse)
library(janitor)
library(broom)
library(rsample)
library(yardstick)
```

## Design and Data Collection

The data was collected from Redfin with Excel. Orange County (OC), Los Angeles County (LA), and Riverside County (RIV) were entered into Redfin's search unter the "Rent" tab. Based on Redfin's search filter, only house, apartment, and condo were selected since a college student is unlikely to be renting a townhome under "hometype." The URL was copied after each separate search. Using the Power Query's "From Web" function, we can connect to the Redfin website with the URL and from there, specific information (such as the aforementioned variables) can be selected before loading our data onto a spreadsheet. I thank Natalie for her help in importing the data as I was limited by MacOS in which Excel did not have this function.

It must be noted that the data was collected from Redfin's recommended hits. There is no way of knowing what algorithm or criteria was used to determine what listings get pushed to "Recommended" unfortunately. It may as well be a combination of all the other sort options, such as price (low to high/high to low), square feet, beds, baths, newest, and/or oldest listings. It is unlikely Redfin will tell us how they choose what gets recommended or not but I would not be surprised if our data was skewed in some way to prefer a specific "type" of listing, whatever that entails. There was also no way to make sure that the data was representative of the entire county, such as performing stratified sampling so that the number of listings from each city was proportional to the city's size in the county.

I acknowledge that the method of data collection may limit the analysis and conclusion but the context of this project is from the perspective of an undergraduate student. If we consider the realities for a majority of this demographic, especially in the financial sense, there is little space to be "picky" and I also expect that not many individuals will know what they want, especially if it is their first time searching. Therefore, unless an individual knows exactly what they want, the recommended listings is the best (although default) sort option for now. The project was designed to be "general" and can be customized according to individual preferences, if any.

```{r}
#| label: load dataset
#| 
socalrent <- read_csv(here::here("project1/socal_raw.csv"))
```

```{r}
#| label: preview dataset

socalrent |>
  group_by(County, Type) |>
    count() # displays number of each type of listing in each county

socalrent |>
  head() # preview first few observations, note the empty columns 

socalrent |>
  slice(265:266, 275:278) # note missing info and strings 

socalrent |>
  filter(
    Type == "house",
    Beds <= 1 # based on previous assumptions, a house should not have 0 bedrooms
  ) 
```

## Data Massaging

Based on what we saw in the previews, there is significant cleaning to be done, especially with strings. Below are the steps taken with an explanation beside somes lines of code.

```{r}
#| label: remove empty columns and rows w/ missing values

clean_socal <- socalrent |>
  select(Price, Beds, Baths, SqFt, Address, Type, County) |> # removes empty columns
  mutate(
    SqFt = str_replace_all(SqFt, "â€”", NA_character_), # changes dashes to NA
  ) |>
  drop_na(c(SqFt, Address)) |> # drop all observations w/ missing info
  filter(
    Address != c("4317 Eileen St, Riverside, CA 92504", "4928 3rd Ave, Los Angeles, CA 90043", "2007 Chubasco Dr Unit A, Corona Del Mar, CA 92625")
  ) # remove the listings that were a house w/ 0 beds based on previews

```

```{r}
#| label: spring (string) cleaning!

clean_socal <- clean_socal |>
  mutate(
    Price = str_remove(Price, ","), # remove commas
    Price = str_extract_all(Price, "[0-9]+"), # only want numerical values
    Price = as.numeric(Price), # turn char -> num
    Beds = str_extract(Beds, "[0-9]+"), # only include 1st element in the range
    Beds = as.numeric(Beds),
    Baths = str_extract(Baths, "\\d+\\.*\\d*"), # includes decimals
    Baths = as.numeric(Baths), 
    SqFt = str_remove(SqFt, ","),
    SqFt = str_extract(SqFt, "[0-9]+"), 
    SqFt = as.numeric(SqFt)
  ) 
```

## Exploratory Data Analysis

Now that our data has been cleaned, it is now suitable for exploratory analysis.

```{r}
#| label: five number summary by County

clean_socal |>
  group_by(County) |>
  summarize(
    Min = min(Price),
    `1st Quartile` = quantile(Price, 0.25),
    Median = median(Price),
    `3rd Quartile` = quantile(Price, 0.75),
    Max = max(Price)
  )
```

```{r}
#| label: analyze Price per SqFt for each Type in each County

clean_socal |>
  mutate(
    price_ratio = Price/SqFt
  ) |>
  group_by(County, Type) |>
  summarize(
    `Mean Price/SqFt` = round(mean(price_ratio), digits = 2),
    `SD Price/SqFt` = round(sd(price_ratio), digits = 2)
  )
```

As we can see from the both chunks, RIV is on the lower end in terms of monthly rent. The reason could be that the area has not gone through much development and has older buildings mainly. It is not surprising to see LA and OC on the higher end of the scale. LA in general is expensive to live in while OC has a number of new developments, notably Irvine. From the first chunk, a monthly rent of \$20000 and \$30000 is outrageous. We'll deal with this in a later step.

```{r}
#| label: visualize data

clean_socal |>
  mutate(
    price_ratio = Price/SqFt
  ) |>
  ggplot(mapping = aes(x = price_ratio)) +
  geom_histogram(
   # center = 1, 
   binwidth = 0.5, 
   fill = "#336699", 
   color = "white"
  ) + labs(
    title = "Distribution of Price/SqFt Ratio in LA, OC, & RIV County Rent Listings",
    x = "Price/SqFt Ratio", 
    y = "Count"
  ) +
  geom_vline(
    xintercept = 6, 
    linetype = "dashed"
  ) +
  geom_vline(
    xintercept = 1,
    linetype = "dashed"
  ) +
  annotate("text", 
           x = 7, 
           y = 17, 
           label = "Observations outside of dashed \nlines should be investigated",
           hjust = 0,
           vjust = 0.5,
           size = 10, 
           size.unit = "pt",
           color = "black")
  
```

The best way to eliminate unusually expensive (or inexpensive) listings is by looking at the Price per SqFt ratio. Obviously, unusually expensive listings should be removed because that would be out of the budget or (unless there's a good reason) we might be getting "ripped off" for lack of a better word. Unusually INEXPENSIVE listings are suspicious, warranting further investigation. For now, based on our judgement calls these listings will be excluded so as to not skew our linear regression model.

```{r}
#| label: visualize (assumed) linear relationship

clean_socal |>
  mutate(
    price_ratio = Price/SqFt
  ) |>
  filter(
    price_ratio >= 1 & price_ratio <= 6
  ) |>
  ggplot(
    mapping = aes(x = SqFt, y = Price)
  ) +
  geom_point() +
  geom_smooth(method = "lm", color = "purple", linetype = "dashed", se = FALSE) +
  labs(
    title = "Relationship between Price vs SqFt"
  )
```

The resulting linear regression model will look like this. The \$20000 listing was not excluded from our filtering. We can continue to remove listings that are outside of our budget but the more we continue to remove, the more skewed our model may be. For the most part however, there is a positive, moderately strong correlation between SqFt as a predictor of Price.

```{r}
#| label: find residuals

linreg <- lm(Price ~ SqFt, data = clean_socal)
  
clean_socal |>
  mutate(
    price_ratio = Price/SqFt,
    pricediff = linreg$residuals
  ) |>
  filter(
    price_ratio >= 1 & price_ratio <= 6,
    pricediff <= 0
  ) |>
  arrange(pricediff)
```

After creating our model, we can now compare the actual monthly rent to the predicted by calculating the residuals. A negative residual would indicate that the actual value is less than the predicted, which I defined as a "deal" earlier. If it was equal to zero, that would mean the actual value matches the predicted.

## Conclusion & Limitations

If I was actually looking for a place to rent, I would choose:

![](images/Screenshot%202025-05-08%20at%2012.54.16.png)

With a starting price of \$1596 for 0 beds (studio apartment) and 1 bath, the calculated residual was approximately \$766. Personally, saving on \$766 on rent is pretty good. Realistically, having a roommate to split the payment is probably a good idea. If I was working full-time for roughly \$18/hr, I would not be opposed to spending half my income on rent but that's just me.

After completing this project, I learned that R is an excellent calculator, especially for handling big amounts of data. I would take advantage of this if I was a real estate agent. However, if I was the average person who did not know how to utilize R, Redfin's built-in search filters are probably better and easier to use.

However, there is more to be considered than just the numbers, especially in the context of this project. There is a strong assumption throughout the project of a linear relationship between Price and SqFt. While SqFt may be a strong predictor of Price, realistically it cannot be the ONLY predictor. While I am driven by numbers, other may also consider the location of the listing and its surroundings (near a school, park, shopping plaza, etc.), public transportation, pet friendliness, appearance of the listing (both exterior and interior), neighborhood safety, amenities, and the list goes on. As stated previously, this project does not account for all variables that are typically considered when searching for a place which is why it remains very "generalized" and is meant to relieve some burden.

We should also address that the dataset is quite literally stuck in time, only displaying information from the day it was collected. Since this project spanned over several weeks, listings have updated in price or have been removed. Interestingly, I found one listing that was supposedly up for rent but when I referenced back on Redfin, it had been bought out since 2019. Several other listings were apparently "off the market."

Furthermore, we have a moral duty to follow Redfin's (or any other site) rules when it comes to web scraping. We should be mindful of data privacy laws and regulations, only handling publicly available information and avoid private or sensitive data. We must also be aware of the frequency that we scrape from sites as it could lead to site overload and crashes, affecting both the site owners and the people who use it. Otherwise, we may be faced with legal consequences (and nobody likes that). When in doubt, review the terms of service prior to performing any web scraping.
